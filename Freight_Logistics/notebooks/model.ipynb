{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directories():\n",
    "    \"\"\"\n",
    "    Create necessary directories if they don't exist\n",
    "    \"\"\"\n",
    "    directories = ['../data/processed', '../models', '../results']\n",
    "    for directory in directories:\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "        print(f\"Created directory: {directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    \"\"\"\n",
    "    Load processed data and split into train/test sets\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing data...\")\n",
    "    \n",
    "    # Load the processed data\n",
    "    df = pd.read_csv('../data/processed/processed_data.csv')\n",
    "    print(\"Loaded processed data shape:\", df.shape)\n",
    "    \n",
    "    # Separate features and target\n",
    "    target = 'Delayed'\n",
    "    features = [col for col in df.columns if col != target]\n",
    "\n",
    "    # Identify non-numeric columns\n",
    "    non_numeric_cols = df[features].select_dtypes(include=['object']).columns.tolist()\n",
    "    if non_numeric_cols:\n",
    "        print(f\"\\nNon-numeric columns detected: {non_numeric_cols}\")\n",
    "\n",
    "    # Drop or encode non-numeric columns\n",
    "    df = pd.get_dummies(df, columns=non_numeric_cols, drop_first=True)\n",
    "\n",
    "    features = [col for col in df.columns if col != target]\n",
    "\n",
    "    X = df[features]\n",
    "    y = df[target]\n",
    "\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, \n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    # Save the split datasets\n",
    "    X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "    X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "    y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "    y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "    \n",
    "    print(\"Train-test split complete:\")\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(X_train):\n",
    "    \"\"\"\n",
    "    Analyze and plot feature correlations\n",
    "    \"\"\"\n",
    "    print(\"\\nAnalyzing feature correlations...\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    correlation_matrix = X_train.corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "    \n",
    "    # Print high correlations\n",
    "    threshold = 0.5\n",
    "    high_corr = np.where(np.abs(correlation_matrix) > threshold)\n",
    "    high_corr = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y])\n",
    "                 for x, y in zip(*high_corr) if x != y and x < y]\n",
    "    \n",
    "    if high_corr:\n",
    "        print(\"\\nHigh correlations (>0.5):\")\n",
    "        for feat1, feat2, corr in high_corr:\n",
    "            print(f\"{feat1} - {feat2}: {corr:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scale numerical features\n",
    "    \"\"\"\n",
    "    print(\"\\nScaling features...\")\n",
    "\n",
    "    # Identify numeric columns for scaling\n",
    "    numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Scale numeric columns\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "\n",
    "    X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "    X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "    \"\"\"\n",
    "    Train and evaluate a single model\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "    print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "    print(f\"ROC AUC: {auc:.3f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "    plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'ROC Curve - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(f'../results/roc_curve_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.savefig(f'../results/confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'accuracy': accuracy,\n",
    "        'auc': auc,\n",
    "        'cv_scores': cv_scores,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple models\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining multiple models...\")\n",
    "    \n",
    "    models = {\n",
    "        'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "        'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "        'Random Forest': RandomForestClassifier(random_state=42),\n",
    "        'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        results[name] = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    \n",
    "    # Find best model\n",
    "    best_model_name = max(results.items(), key=lambda x: x[1]['accuracy'])[0]\n",
    "    print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    \n",
    "    return results, best_model_name\n",
    "\n",
    "def optimize_best_model(best_model_name, results, X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter optimization on the best model\n",
    "    \"\"\"\n",
    "    print(f\"\\nOptimizing {best_model_name}...\")\n",
    "    \n",
    "    best_model = results[best_model_name]['model']\n",
    "    \n",
    "    if isinstance(best_model, RandomForestClassifier):\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    elif isinstance(best_model, GradientBoostingClassifier):\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    else:\n",
    "        print(\"Optimization skipped - no parameter grid defined for this model type\")\n",
    "        return best_model\n",
    "    \n",
    "    grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "    \n",
    "    # Evaluate optimized model\n",
    "    optimized_results = evaluate_model(\n",
    "        grid_search.best_estimator_,\n",
    "        X_train, X_test, y_train, y_test,\n",
    "        f\"Optimized {best_model_name}\"\n",
    "    )\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(model, scaler, model_name):\n",
    "    \"\"\"\n",
    "    Save the trained model and scaler\n",
    "    \"\"\"\n",
    "    print(\"\\nSaving models...\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_filename = f'../models/optimized_{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "    joblib.dump(model, model_filename)\n",
    "    print(f\"Saved model: {model_filename}\")\n",
    "    \n",
    "    # Save the scaler\n",
    "    scaler_filename = '../models/scaler.pkl'\n",
    "    joblib.dump(scaler, scaler_filename)\n",
    "    print(f\"Saved scaler: {scaler_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model development pipeline...\n",
      "Created directory: ../data/processed\n",
      "Created directory: ../models\n",
      "Created directory: ../results\n",
      "\n",
      "Preparing data...\n",
      "Loaded processed data shape: (20000, 23)\n",
      "\n",
      "Non-numeric columns detected: ['Shipment_Date', 'Planned_Delivery_Date', 'Actual_Delivery_Date']\n",
      "Train-test split complete:\n",
      "Training set: 16000 samples\n",
      "Testing set: 4000 samples\n",
      "\n",
      "Analyzing feature correlations...\n",
      "\n",
      "High correlations (>0.5):\n",
      "Distance_(km) - Planned_Delivery_Days: 0.96\n",
      "Distance_(km) - Actual_Delivery_Days: 0.64\n",
      "Distance_(km) - Is_Long_Distance: 0.87\n",
      "Weather_Conditions - Is_Bad_Weather: 0.91\n",
      "Traffic_Conditions - Is_Heavy_Traffic: -0.75\n",
      "Planned_Delivery_Days - Actual_Delivery_Days: 0.62\n",
      "Planned_Delivery_Days - Is_Long_Distance: 0.84\n",
      "Actual_Delivery_Days - Delay_Days: 0.74\n",
      "Actual_Delivery_Days - Is_Long_Distance: 0.55\n",
      "Delay_Days - Avg_Speed: -0.78\n",
      "Shipment_Month - Shipment_Quarter: 0.97\n",
      "Shipment_Day_Of_Week - Is_Weekend: 0.79\n",
      "Planned_Delivery_Date_2023-01-03 - Actual_Delivery_Date_2023-01-03: 0.54\n",
      "\n",
      "Scaling features...\n",
      "\n",
      "Training multiple models...\n",
      "\n",
      "Evaluating Logistic Regression...\n",
      "\n",
      "Logistic Regression Results:\n",
      "--------------------\n",
      "Cross-validation accuracy: 0.997 (+/- 0.002)\n",
      "Test accuracy: 0.999\n",
      "ROC AUC: 1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1049\n",
      "           1       1.00      1.00      1.00      2951\n",
      "\n",
      "    accuracy                           1.00      4000\n",
      "   macro avg       1.00      1.00      1.00      4000\n",
      "weighted avg       1.00      1.00      1.00      4000\n",
      "\n",
      "\n",
      "Evaluating Decision Tree...\n",
      "\n",
      "Decision Tree Results:\n",
      "--------------------\n",
      "Cross-validation accuracy: 1.000 (+/- 0.000)\n",
      "Test accuracy: 1.000\n",
      "ROC AUC: 1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1049\n",
      "           1       1.00      1.00      1.00      2951\n",
      "\n",
      "    accuracy                           1.00      4000\n",
      "   macro avg       1.00      1.00      1.00      4000\n",
      "weighted avg       1.00      1.00      1.00      4000\n",
      "\n",
      "\n",
      "Evaluating Random Forest...\n",
      "\n",
      "Random Forest Results:\n",
      "--------------------\n",
      "Cross-validation accuracy: 1.000 (+/- 0.000)\n",
      "Test accuracy: 1.000\n",
      "ROC AUC: 1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1049\n",
      "           1       1.00      1.00      1.00      2951\n",
      "\n",
      "    accuracy                           1.00      4000\n",
      "   macro avg       1.00      1.00      1.00      4000\n",
      "weighted avg       1.00      1.00      1.00      4000\n",
      "\n",
      "\n",
      "Evaluating Gradient Boosting...\n",
      "\n",
      "Gradient Boosting Results:\n",
      "--------------------\n",
      "Cross-validation accuracy: 1.000 (+/- 0.000)\n",
      "Test accuracy: 1.000\n",
      "ROC AUC: 1.000\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      1049\n",
      "           1       1.00      1.00      1.00      2951\n",
      "\n",
      "    accuracy                           1.00      4000\n",
      "   macro avg       1.00      1.00      1.00      4000\n",
      "weighted avg       1.00      1.00      1.00      4000\n",
      "\n",
      "\n",
      "Best performing model: Decision Tree\n",
      "\n",
      "Optimizing Decision Tree...\n",
      "Optimization skipped - no parameter grid defined for this model type\n",
      "\n",
      "Saving models...\n",
      "Saved model: ../models/optimized_decision_tree.pkl\n",
      "Saved scaler: ../models/scaler.pkl\n",
      "\n",
      "Model development pipeline completed!\n",
      "Check the 'results' directory for plots and the 'models' directory for saved models.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"\n",
    "    Main execution pipeline\n",
    "    \"\"\"\n",
    "    print(\"Starting model development pipeline...\")\n",
    "    \n",
    "    # Create necessary directories\n",
    "    create_directories()\n",
    "    \n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test = prepare_data()\n",
    "    \n",
    "    # Analyze correlations\n",
    "    analyze_correlations(X_train)\n",
    "    \n",
    "    # Scale features\n",
    "    X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "    \n",
    "    # Train and evaluate models\n",
    "    results, best_model_name = train_models(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    \n",
    "    # Optimize best model\n",
    "    optimized_model = optimize_best_model(\n",
    "        best_model_name, results, \n",
    "        X_train_scaled, X_test_scaled, \n",
    "        y_train, y_test\n",
    "    )\n",
    "    \n",
    "    # Save models\n",
    "    save_models(optimized_model, scaler, best_model_name)\n",
    "    \n",
    "    print(\"\\nModel development pipeline completed!\")\n",
    "    print(\"Check the 'results' directory for plots and the 'models' directory for saved models.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import joblib\n",
    "# from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix,\n",
    "#                            roc_curve, roc_auc_score)\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# def create_directories():\n",
    "#     \"\"\"\n",
    "#     Create necessary directories if they don't exist\n",
    "#     \"\"\"\n",
    "#     directories = ['../data/processed', '../models', '../results']\n",
    "#     for directory in directories:\n",
    "#         os.makedirs(directory, exist_ok=True)\n",
    "#         print(f\"Created directory: {directory}\")\n",
    "\n",
    "# def prepare_data():\n",
    "#     \"\"\"\n",
    "#     Load processed data and split into train/test sets\n",
    "#     \"\"\"\n",
    "#     print(\"\\nPreparing data...\")\n",
    "    \n",
    "#     # Load the processed data\n",
    "#     df = pd.read_csv('data/processed/processed_data.csv')\n",
    "#     print(\"Loaded processed data shape:\", df.shape)\n",
    "    \n",
    "#     # Separate features and target\n",
    "#     target = 'Delayed'\n",
    "#     features = [col for col in df.columns if col != target]\n",
    "\n",
    "#     # Identify non-numeric columns\n",
    "#     non_numeric_cols = df[features].select_dtypes(include=['object']).columns.tolist()\n",
    "#     if non_numeric_cols:\n",
    "#         print(f\"\\nNon-numeric columns detected: {non_numeric_cols}\")\n",
    "\n",
    "#     # Drop or encode non-numeric columns\n",
    "#     df = pd.get_dummies(df, columns=non_numeric_cols, drop_first=True)\n",
    "\n",
    "#     features = [col for col in df.columns if col != target]\n",
    "\n",
    "#     X = df[features]\n",
    "#     y = df[target]\n",
    "\n",
    "#     # Split the data\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(\n",
    "#         X, y, \n",
    "#         test_size=0.2,\n",
    "#         random_state=42,\n",
    "#         stratify=y\n",
    "#     )\n",
    "    \n",
    "#     # Save the split datasets\n",
    "#     X_train.to_csv('../data/processed/X_train.csv', index=False)\n",
    "#     X_test.to_csv('../data/processed/X_test.csv', index=False)\n",
    "#     y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "#     y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "    \n",
    "#     print(\"Train-test split complete:\")\n",
    "#     print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "#     print(f\"Testing set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "#     return X_train, X_test, y_train, y_test\n",
    "\n",
    "# def analyze_correlations(X_train):\n",
    "#     \"\"\"\n",
    "#     Analyze and plot feature correlations\n",
    "#     \"\"\"\n",
    "#     print(\"\\nAnalyzing feature correlations...\")\n",
    "    \n",
    "#     plt.figure(figsize=(12, 8))\n",
    "#     correlation_matrix = X_train.corr()\n",
    "#     sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "#     plt.title('Feature Correlation Matrix')\n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('../results/correlation_matrix.png')\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Print high correlations\n",
    "#     threshold = 0.5\n",
    "#     high_corr = np.where(np.abs(correlation_matrix) > threshold)\n",
    "#     high_corr = [(correlation_matrix.index[x], correlation_matrix.columns[y], correlation_matrix.iloc[x, y])\n",
    "#                  for x, y in zip(*high_corr) if x != y and x < y]\n",
    "    \n",
    "#     if high_corr:\n",
    "#         print(\"\\nHigh correlations (>0.5):\")\n",
    "#         for feat1, feat2, corr in high_corr:\n",
    "#             print(f\"{feat1} - {feat2}: {corr:.2f}\")\n",
    "\n",
    "# def scale_features(X_train, X_test):\n",
    "#     \"\"\"\n",
    "#     Scale numerical features\n",
    "#     \"\"\"\n",
    "#     print(\"\\nScaling features...\")\n",
    "\n",
    "#     # Identify numeric columns for scaling\n",
    "#     numeric_cols = X_train.select_dtypes(include=['int64', 'float64']).columns\n",
    "#     scaler = StandardScaler()\n",
    "\n",
    "#     # Scale numeric columns\n",
    "#     X_train_scaled = X_train.copy()\n",
    "#     X_test_scaled = X_test.copy()\n",
    "\n",
    "#     X_train_scaled[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
    "#     X_test_scaled[numeric_cols] = scaler.transform(X_test[numeric_cols])\n",
    "    \n",
    "#     return X_train_scaled, X_test_scaled, scaler\n",
    "\n",
    "# def evaluate_model(model, X_train, X_test, y_train, y_test, model_name):\n",
    "#     \"\"\"\n",
    "#     Train and evaluate a single model\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nEvaluating {model_name}...\")\n",
    "    \n",
    "#     # Train the model\n",
    "#     model.fit(X_train, y_train)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     y_pred = model.predict(X_test)\n",
    "#     y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "#     # Calculate metrics\n",
    "#     cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "#     accuracy = accuracy_score(y_test, y_pred)\n",
    "#     auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "#     # Print results\n",
    "#     print(f\"\\n{model_name} Results:\")\n",
    "#     print(\"-\" * 20)\n",
    "#     print(f\"Cross-validation accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "#     print(f\"Test accuracy: {accuracy:.3f}\")\n",
    "#     print(f\"ROC AUC: {auc:.3f}\")\n",
    "#     print(\"\\nClassification Report:\")\n",
    "#     print(classification_report(y_test, y_pred))\n",
    "    \n",
    "#     # Plot ROC curve\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "#     plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "#     plt.plot([0, 1], [0, 1], 'k--')\n",
    "#     plt.xlim([0.0, 1.0])\n",
    "#     plt.ylim([0.0, 1.05])\n",
    "#     plt.xlabel('False Positive Rate')\n",
    "#     plt.ylabel('True Positive Rate')\n",
    "#     plt.title(f'ROC Curve - {model_name}')\n",
    "#     plt.legend(loc=\"lower right\")\n",
    "#     plt.savefig(f'results/roc_curve_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "#     plt.close()\n",
    "    \n",
    "#     # Plot confusion matrix\n",
    "#     plt.figure(figsize=(8, 6))\n",
    "#     cm = confusion_matrix(y_test, y_pred)\n",
    "#     sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "#     plt.title(f'Confusion Matrix - {model_name}')\n",
    "#     plt.ylabel('True Label')\n",
    "#     plt.xlabel('Predicted Label')\n",
    "#     plt.savefig(f'results/confusion_matrix_{model_name.lower().replace(\" \", \"_\")}.png')\n",
    "#     plt.close()\n",
    "    \n",
    "#     return {\n",
    "#         'model': model,\n",
    "#         'accuracy': accuracy,\n",
    "#         'auc': auc,\n",
    "#         'cv_scores': cv_scores,\n",
    "#         'predictions': y_pred,\n",
    "#         'probabilities': y_pred_proba\n",
    "#     }\n",
    "\n",
    "# def train_models(X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"\n",
    "#     Train and evaluate multiple models\n",
    "#     \"\"\"\n",
    "#     print(\"\\nTraining multiple models...\")\n",
    "    \n",
    "#     models = {\n",
    "#         'Logistic Regression': LogisticRegression(max_iter=1000),\n",
    "#         'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "#         'Random Forest': RandomForestClassifier(random_state=42),\n",
    "#         'Gradient Boosting': GradientBoostingClassifier(random_state=42)\n",
    "#     }\n",
    "    \n",
    "#     results = {}\n",
    "#     for name, model in models.items():\n",
    "#         results[name] = evaluate_model(model, X_train, X_test, y_train, y_test, name)\n",
    "    \n",
    "#     # Find best model\n",
    "#     best_model_name = max(results.items(), key=lambda x: x[1]['auc'])[0]\n",
    "#     print(f\"\\nBest performing model: {best_model_name}\")\n",
    "    \n",
    "#     return results, best_model_name\n",
    "\n",
    "# def optimize_best_model(best_model_name, results, X_train, X_test, y_train, y_test):\n",
    "#     \"\"\"\n",
    "#     Perform hyperparameter optimization on the best model\n",
    "#     \"\"\"\n",
    "#     print(f\"\\nOptimizing {best_model_name}...\")\n",
    "    \n",
    "#     best_model = results[best_model_name]['model']\n",
    "    \n",
    "#     if isinstance(best_model, RandomForestClassifier):\n",
    "#         param_grid = {\n",
    "#             'n_estimators': [100, 200, 300],\n",
    "#             'max_depth': [10, 20, 30, None],\n",
    "#             'min_samples_split': [2, 5, 10],\n",
    "#             'min_samples_leaf': [1, 2, 4]\n",
    "#         }\n",
    "#     elif isinstance(best_model, GradientBoostingClassifier):\n",
    "#         param_grid = {\n",
    "#             'n_estimators': [100, 200, 300],\n",
    "#             'learning_rate': [0.01, 0.1, 0.3],\n",
    "#             'max_depth': [3, 4, 5],\n",
    "#             'min_samples_split': [2, 5, 10]\n",
    "#         }\n",
    "#     else:\n",
    "#         print(\"Optimization skipped - no parameter grid defined for this model type\")\n",
    "#         return best_model\n",
    "    \n",
    "#     grid_search = GridSearchCV(best_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "#     grid_search.fit(X_train, y_train)\n",
    "    \n",
    "#     print(\"\\nBest parameters:\", grid_search.best_params_)\n",
    "#     print(\"Best cross-validation accuracy:\", grid_search.best_score_)\n",
    "    \n",
    "#     # Evaluate optimized model\n",
    "#     optimized_results = evaluate_model(\n",
    "#         grid_search.best_estimator_,\n",
    "#         X_train, X_test, y_train, y_test,\n",
    "#         f\"Optimized {best_model_name}\"\n",
    "#     )\n",
    "    \n",
    "#     return grid_search.best_estimator_\n",
    "\n",
    "# def save_models(model, scaler, model_name):\n",
    "#     \"\"\"\n",
    "#     Save the trained model and scaler\n",
    "#     \"\"\"\n",
    "#     print(\"\\nSaving models...\")\n",
    "    \n",
    "#     # Save the model\n",
    "#     model_filename = f'models/optimized_{model_name.lower().replace(\" \", \"_\")}.pkl'\n",
    "#     joblib.dump(model, model_filename)\n",
    "#     print(f\"Saved model: {model_filename}\")\n",
    "    \n",
    "#     # Save the scaler\n",
    "#     scaler_filename = 'models/scaler.pkl'\n",
    "#     joblib.dump(scaler, scaler_filename)\n",
    "#     print(f\"Saved scaler: {scaler_filename}\")\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Main execution pipeline\n",
    "#     \"\"\"\n",
    "#     print(\"Starting model development pipeline...\")\n",
    "    \n",
    "#     # Create necessary directories\n",
    "#     create_directories()\n",
    "    \n",
    "#     # Prepare data\n",
    "#     X_train, X_test, y_train, y_test = prepare_data()\n",
    "    \n",
    "#     # Analyze correlations\n",
    "#     analyze_correlations(X_train)\n",
    "    \n",
    "#     # Scale features\n",
    "#     X_train_scaled, X_test_scaled, scaler = scale_features(X_train, X_test)\n",
    "    \n",
    "#     # Train and evaluate models\n",
    "#     results, best_model_name = train_models(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "    \n",
    "#     # Optimize best model\n",
    "#     optimized_model = optimize_best_model(\n",
    "#         best_model_name, results, \n",
    "#         X_train_scaled, X_test_scaled, \n",
    "#         y_train, y_test\n",
    "#     )\n",
    "    \n",
    "#     # Save models\n",
    "#     save_models(optimized_model, scaler, best_model_name)\n",
    "    \n",
    "#     print(\"\\nModel development pipeline completed!\")\n",
    "#     print(\"Check the 'results' directory for plots and the 'models' directory for saved models.\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
